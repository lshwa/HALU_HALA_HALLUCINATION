# BERT

**Bidirectional Encoder Representations from Transformers**



## 자연어 처리의 발전 흐름 요약

사전 훈련된 워드 임베딩 (Word2Vec, GloVe, FastText)

**개념** : 단어를 고정된 벡터로 표현하는 방법

**사용 방식** :

1. 임베딩 레이어를 랜덤 초기화해 직접 학습
2. 대규모 데이터로 **사전 학습된 임베딩**을 가져와서 사용

**장점** : 데이터가 적을 때도 성능 향상이 가능하다.

**한계** : 단어 하나당 하나의 벡터 -> **문맥 정보 반영 불가** , 다의어 구분이 불가능



**Pre-trained Language Model**

**아이디어** : 라벨 없이 대규모 텍스트로 '언어 모델(다음 단어 예측)' 을 먼저 학습하고, 이후 **추가 학습(fine-tuning)**으로 특정 태스크 (분류, 번역 등)에 적용

- 초기 시도 : 2015, Semi-supervised Sequence Learning
  - LSTM 언어모델을 하급하여 텍스트 분류 태스크에 전이 
  - 사전 훈련된 모델이 랜덤 초기화보다 성능이 우수함을 입증



**ELMo**

**구조** : 순방향 LSTM + 역방향 LSTM 두 개를 각각 학습

핵심 특징:

- 문맥에 따라 단어 임베딩이 달라지기에 동일 단어라도 위치, 문맥별로 다르다.
- 다의어 문제 해결이 가능하다.

**한계** : 여전히 RNN 기반하여 순차적 처리로 속도와 확장성의 한계가 존재한다. 

> 이는 **트랜스포머**의 등장으로 RNN의 제약을 극복했다. 



**GPT**

**구조** : Tranformer의 **Decoder 블록만 사용**

학습 방식 : 이전 단어들로 다음 단어 예측

특징:

- 12층 Transformer Decoder 
- 방대한 텍스트 사전 학습 + 태스크별 추가학습으로 높은 성능 달성

**의의** : 사전학습 + 파인튜닝 시대의 시작



**양방향 언어 모델의 필요성**

- 기존 언어모델(GPT, LSTM)은 **한쪽 방향(과거→미래)** 으로만 단어를 예측.

- 하지만 문맥은 실제로 **양방향**임.

  예) “나는 [MASK]에 갔다” → 앞뒤 문맥 모두 중요.

- **문제**:

  양방향 LSTM 언어 모델은 미래 단어를 “이미 본 상태”에서 예측하게 되어

  실제 학습 불가능.

➡ **ELMo**는 순·역방향 두 모델을 따로 학습해서 부분적으로 해결했지만,

완전한 양방향 처리는 불가능했음.



**MLM (Masked Language Model)**

- **아이디어**: 문장 중 일부 단어(약 15%)를 [MASK]로 가리고 모델이 **빈칸을 채우도록 예측**하게 함.

- **장점**:
  - 문장 전체(좌·우 문맥)를 모두 활용 → **양방향 문맥 학습 가능**
  - 별도의 레이블 없이도 대규모 학습 가능

➡ 이 방식을 채택한 모델이 바로 **BERT (2018)**.



## BERT 

### 토크나이저 / 어휘 세부

- Cased vs Uncased: bert-base-uncased는 소문자화(대소문자 정보 손실), bert-base-cased는 보존. 개체명(인명/지명) 비중이 크면 cased가 유리한 경우가 있음.
- Whole-Word Masking(WWM): WordPiece로 쪼개진 서브워드가 하나의 “단어”를 이루면 그 단어 전체를 동시에 마스킹. 문맥 난이도↑ → 학습 안정·성능 개선 사례 다수.
- 한국어: 교착어 특성상 서브워드 분절이 길어지는 경향. 한국어 특화 사전(mBERT vs KoBERT vs KR-BERT) 간 어휘 커버리지 차이가 파인튜닝 효율에 영향.



### 입력 구성 실전 팁

- 길이 제한(512 토큰): 긴 문서는 슬라이딩 윈도우 또는 문단 추출(텍스트 랭킹/BM25/RAG 전처리)로 축약해 입력.
- [CLS] 활용: 분류는 보통 [CLS]의 최종 히든 벡터를 사용하지만, **mean/max pooling**(토큰 평균/최댓값)이나 **[CLS]+Attn pooling**이 더 안정적인 태스크도 있음.
- Attention Mask: 패딩 위치를 0으로 마스킹해 불필요한 연산/누설 방지. 긴 문서에서 마스크 정확도가 성능·속도 모두에 영향.



### 사전학습 관련

- NSP 재고: 후속 연구(RoBERTa 등)는 NSP 제거(혹은 SOP로 대체) + 더 긴 학습/큰 배치/동적 마스킹으로 성능 향상.
- 동적 마스킹(Dynamic Masking): 에폭마다 마스크 위치를 바꿔 과적합·패턴 학습을 줄임(고정 마스킹 대비 일반화↑).
- 손실과 퍼플렉서티: BERT는 **MLM 손실**을 최소화하며, 전통적 LM처럼 문장 전체 PPL을 직접 쓰기 어렵다(마스크 기반).



### 학습, 최적화 (파인튜닝)

- 옵티마이저: AdamW(가중치 감쇠를 옵티마이저 분리 적용).
- 러닝레이트: 1e-5 ~ 5e-5에서 탐색, **Warmup(예: 10%) + Linear decay**가 표준.
- 레이어별 LR 디케이(LLRD): 하단 인코더는 작게, 상단과 헤드는 크게(예: 1.0, 0.95, 0.9… 비율).
- 드롭아웃 0.1, Gradient Clipping(예: 1.0), Mixed Precision(FP16)으로 속도/메모리 최적화.
- 프리징 전략: 데이터가 매우 작을 때 하위 레이어 동결 + 상위/헤드만 학습 → 과적합 완화.
- 배치·시퀀스 길이의 트레이드오프: 길이를 줄이면 배치를 키울 수 있어 통계적 안정성↑.



### 태스크별 헤드 / 설계 Detail

- NER/태깅: 토큰별 로짓 → 필요 시 CRF 레이어로 전이 제약(“BIO” 일관성) 반영. 서브워드의 라벨링은 **첫 토큰만 라벨** 부여가 일반적.
- 문장쌍 과제(NLI/STS 등): [CLS] 기반 분류 외에 **[SEP] 전후 평균 풀링 결합**이나 **학습 가능한 pairwise attention**으로 소폭 개선되는 경우 존재.
- QA(Span 추출): 시작/끝 위치 이중 헤드. 긴 본문은 슬라이딩 윈도우 + 문단 선택(리트리버)로 효율화.



### 한계와 개선 계열

- 길이 복잡도: Self-Attention O(n²) → Longformer/BigBird/ETC 등으로 장문 대응.
- 사전학습 목표의 진화: ELECTRA(Discriminator 방식), DeBERTa(상대적 위치·디센탱글 임베딩), RoBERTa(NSP 제거+대규모 학습).
- 경량화: DistilBERT(지식증류), ALBERT(파라미터 공유)로 추론 지연/메모리 절감.



> 평가 디버깅 포인트 
>
> - 랜덤 시드/데이터 분할 고정: 작은 데이터셋에서 분산 큼.
> - 검증 로그: 학습 초기 **훈련↑/검증↔**이면 LR 과대, **둘 다 정체**면 과소.
> - 에러분석: 오답 샘플의 서브워드 분절/문맥 길이/고유명사 케이스를 표로 정리 → 토크나이저·길이·프롬프트 조정.





## SBERT (Sentence - BERT)

> 기존 BERT로부터 문장 임베딩을 받는 세가지 방식
>
> - **[CLS] 벡터 사용**: 마지막 레이어의 [CLS] 출력을 문장 표현으로 사용.
>
> - **Mean Pooling**: 마지막 레이어의 토큰(hidden) 전체를 평균.
>
> - **Max Pooling**: 마지막 레이어의 토큰(hidden) 전체를 차원별 최댓값으로 집약.
>
>   → Mean은 문장 전체 의미를 고르게 반영, Max는 “강한 단서(핵심 토큰)”를 더 강조.



**아이디어**

기존 BERT는 문장 간 유사도 계산 시 느리고 비효율적(문장쌍을 매번 합쳐 인퍼런스).**SBERT는 쌍 입력 대신 문장 단독 임베딩을 직접 학습**하여

- 문장을 **독립적으로 벡터화(offline 인덱싱 가능)**
- 쌍별 유사도는 **코사인(또는 내적)** 한 번으로 처리

즉, “문장 → 고정길이 임베딩 → 코사인 유사도” 파이프라인을 학습으로 직접 최적화



1. 문장쌍 분류(NLI) 로 파인튜닝

1. 문장 A, B를 각각 SBERT로 인코딩 → 임베딩 u, v

2. 특징 결합: 
   $$
   [u;\,v;\,|u-v|] (또는 [u;\,v;\,u\odot v])
   $$

3. 소프트맥스로 **Entailment/Contradiction/Neutral** 분류 학습

   → 분류 헤드 제거 후, 백본 인코더의 **문장 임베딩**만 사용

   장점: 레이블 풍부할 때 강력, 의미적 경계가 또렷함.



2. 문장쌍 회귀 (STS)로 파인튜닝 

1. A, B → 임베딩 u, v

2. **코사인 유사도** \cos(u,v)를 예측 값으로 사용

3. 레이블 유사도(예: 0~5)를 정규화(예: /5) → **MSE** 최소화

   장점: 연속적 유사도(미세한 의미 차) 반영에 유리.



**임베딩 설계**

- **Pooling 레이어**: 마지막 레이어만 쓰거나, 상위 여러 레이어를 가중 평균(Scalar Mix)해 Mean/Max Pooling 적용.
- **정규화**: 임베딩 L2 정규화 후 코사인 사용이 검색에서 일관성↑.
- **손실 대안**: 분류/MSE 외에 **Triplet loss**, **Multiple Negatives Ranking Loss(MNRL)** 를 쓰면 대규모 대조학습(contrastive)로 검색 성능 향상.



**추론, 검색 파이프라인**

1. 말뭉치(문서/문장)를 SBERT로 임베딩하여 **벡터 인덱스(예: FAISS)** 구축
2. 질의문 임베딩 후 **최근접 탐색(k-NN)**
3. 필요 시 **Cross-Encoder**(문장쌍 BERT)로 상위 k개 재랭크 → 정확도↑, 지연은 k에만 비례

