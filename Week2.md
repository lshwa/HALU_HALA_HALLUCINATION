# 딥러닝을 이용한 자연어 처리 입문

> 2주차. 9장 & 12장을 위주로 공부하되, 사이에 있는 챕터도 열심히 읽어본다... 

---

## 워드 임베딩 (Word Embedding)

**워드 임베딩** 

- **단어를 벡터로 표현하는 방법**



### 09-01. 워드 임베딩

#### 1. 희소 표현 (Sparse Representation)

- 원-핫 인코딩을 통해 나온 원-핫 벡터들은 표현하고자 하는 단어의 인덱스의 값만 1이고, 나머지는 전부 0으로 표현하는 방법. 
- 이렇게 벡터 또는 행렬 값이 대부분 0으로 표현되는 방법을 **희소 표현**이라고 함. 
- 차원이 커질수록 **공간적 낭비가 큼**.



#### 2. 밀집 표현 (Dense Representation)

- 벡터의 차원을 단어 집합의 크기로 상정하지 않음. 
- 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춤. 
- 밀집표현의 차원을 128로 설정하여, 모든 벡터 내의 값이 실수가 됨.



#### 3. 워드 임베딩 (Word Embedding)

- **단어를 밀집 벡터의 형태로 표현하는 방법**
- 워드 임베딩 과정을 통해 나온 밀집 벡터 => **임베딩 벡터 (embedding vector)**

<!-- 이미지 1 추가 --> 



### 09-02. 워드투벡터 (Word2Vec)

원-핫 벡터는 벡터 간 유의미한 유사도를 계산할 수 없다는 단점이 존재

> 벡터 간 유의미한 유사도를 반영할 수 있는 단어의 의미를 수치화할 수 있는 방법이 필요함. => **Word2Vec**



#### 1. 희소 표현 (Sparse Representation)

- 앞서 말한 부분 (벡터 또는 행렬의 대부분이 0으로 표현됨)
- 대안 
  - 단어의 의미를 다차원 공간에 벡터화하는 방법 : **분산 표현 (Distributed Representation)**



#### 2. 분산 표현 (Distributed Representation)

- **비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다.** 라는 분포 가설의 가정에서 만들어진것. 
- 분포 가설을 활용해 텍스트를 학습하고, 단어의 의미를 벡터의 여러 차원에 분산하여 표현 



> - 희소표현 : 고차원에 각 차원이 분리된 표현 방법
> - 분산표현 : 저차원에 **단어의 의미를 여러 차원에다가 분산**하여 표현하고, 이런 표현 방법을 통해 **단어 벡터 간 유의미한 유사도**를 계산할 수 있음. 

=> 이렇게 만들어진 표현 방법이 **Word2Vec**임.



#### 3. CBOW (Continuous Bag of Words)

**Word2Vec의 학습 방식** 2가지 : CBOW, Skip-Gram

> 두 개의 메커니즘 자체는 거의 동일함. 



**CBOW**

- 주변에 있는 단어들을 입력으로 중간에 있는 단어들을 예측하는 방법

- 예측해야 하는 단어 : 중심 단어 (center word)
- 예측에 사용되는 단어들을 주변 단어 (context word)
- 앞 뒤로 몇 개의 단어를 볼지 **윈도우**를 정하여 계산함. 
  - 윈도우 크기가 정해지면 윈도우를 옆으로 움직이며 단어의 선택을 번갈아가며 학습을 위한 데이터셋을 만듬. 
  - 이 방법을 **슬라이딩 윈도우 (Sliding Window)**라고 함. 



<!-- 이미지 2, 3추가 -->



(CBOW의 인공 신경망)

- 입력층은 입력으로서 앞, 뒤로 사용자가 정한 윈도우 크기 범위 안에 있는 주변 단어들의 원,핫 벡터가 들어가게 됨.
- 출력층에서 예측하고자 하는 중간 단어의 원-핫 벡터가 레이블로서 필요함. 
- 은닉층이 다순인 딥러닝 모델이 아닌, 1개인 얕은 신경망이다. 
- 활성화 함수가 존재하지 않고, 룩업 테이블이라는 연산을 담당하는 투사층이라고 부르기도 함. 
- 동작 메커니즘

1. **투사층의 크기가 M이라는 점**
   - M은 임베딩하고 난 벡터의 차원이 된다.
2. **입력층과 투사층 사이의 가중치 W는 `V X M`행렬이며, 투사층에서 출력층 사이의 가중치 W'는 `M X V`행렬이라는 점이다.**
   - V는 단어 집합의 크기
   - 두 행렬은 동일한 행렬의 Transpose 가 아니라, 서로 다른 행렬

3. 인공 신경망 이전에 가중치 행렬은 랜덤 값을 가지게 되는데, 주변 단어로 중심 단어를 더 정확히 맞추기 위해 계속 W와 W'을 학습해가는 구조 

4. 구해진 평균 벡터는 두번째 가중치 행렬인 W'와 곱해지면서 소프트맥스 함수를 지나면서 벡터의 간 원소들의 벡터 값은 0과 1 사이의 실수로 총 합은 1이 된다. 
   - 여기서 만들어진 게 **스코어 벡터**
   - 스코어 벡터 : **다중 클래스 분류 문제를 위한 일종의 스코어 벡터**
   - 레이블에 해당하는 벡터인 중심 단어 원-핫 벡터의 값에 가까워져야 한다. 
   - 이 두 벡터값의 오차를 줄이기 위해 손실 함수로 크로스 엔트로피 함수를 사용하여, Input으로 원-핫 벡터와 스코어 벡터를 넣는다. 
     - 역전파를 수행하면 W와 W' 가 학습이 됨. 



#### 4. Skip-gram

**Skip-Gram** 

- 중간에 있는 단어들을 입력으로 주변 단어들을 예측하는 방법 

<!-- 이미지 4 추가 -->



#### 5. NNLM vs Word2Vec

**NNLM** : 단어 벡터 간 유사도를 구할 수 있는 워드 임베딩 개념을 도입

- 다음 단어를 예측하는 모델 
- 예측 단어의 이전 단어들만 참고

- 수식

$$
NNLM :  (n \times m) + (n \times m \times h) + (h \times V)
 
$$



**Word2Vec** : 워드 임베딩 자체에 집중하여 NNLM의 느린 학습 속도와 정확도를 개선하여 탄생

- 다음 단어가 아닌 중심 단어를 예측하여 학습 
- 예측 단어의 전, 후 단어들을 모두 참고
- 수식

$$
Word2Vec : (n \times m) + (m \times \log(V))
$$



 ### 09-03. 영어 / 한국어 Word2Vec 실습

~~~
파이썬의 genism 패키지에서 Word2Vec 지원
- 훈련 데이터는 xml 문법으로 html에서 <content> 내 내용만 가져오면 됨. 

# Word2Vec 의 하이퍼 파라미터 값
vector_size : 워드 벡터의 특징 값, 임베딩 된 벡터의 차원
window : 컨텍스트 윈도우 크기
min_count : 단어 최소 빈도 수 제한 
workers = 학습을 위한 프로세스 수
sg = 0은 CBOW, 1은 Skip-gram 
~~~



### 09-04. 네거티브 샘플링을 이용한 Word2Vec 구현 (Skip-Gram with Negative Sampling, SGNS)

#### 네거티브 샘플링 (Negative Sampling)

- Word2Vec의 출력층에서 소프트맥수 함수를 지닌 단어 집합 크기의 벡터와 실제값인 원-핫 벡터와의 오차를 구하고 이를 통해 임베딩 테이블에 있는 모든 단어에 대한 임베딩 벡터 값을 업데이트 함. 
  - 단어집합이 클 수록, Word2Vec는 **무거운 모델**이 됨. 
- **네거티브 샘플링** : 학습 과정에서 전체 단어 집합이 아닌 일부 단어에만 집중할 수 있도록 하는 방법
  - 이진분류로 긍정, 부정을 레이블링하여 데이터셋을 만든다. 



#### 2. 네거티브 샘플링 Skip-Gram (SGNS)

- 중심 단어와 주변 단어가 모두 입력, 이 두 단어가 실제로 윈도우 크기 내에 존재하는 이웃 관계인지 **확률을 예측**

<!-- 이미지 5 추가 -->



- 입력 1과 입력 2가 실제로 윈도우 크기 내에서 이웃 관계인 경우에 레이블에 1, 아닌 경우에 레이블에 0을 추가함. 

> - 테이블 1 : 입력 1인 중심 단어의 테이블 룩업
> - 테이블 2 : 주변 단어의 테이블 룩업을 위한 임베딩 테이블 

- 중심 단어와 주변 단어의 내적값을 이 모델의 예측값으로 지정
  - 레이블과의 오차로부터 역전파하여 중심 단어와 주변 단어의 임베딩 벡터값을 업데이트 함. 
- 학습 후에는 좌측의 임베딩 행렬을 임베딩 벡터로 사용이 가능, 두 행렬을 더하거나, concatenate 해서 사용도 가능 



#### 구현하는 라이브러리

~~~python
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Embedding, Reshape, Activation, Input
from tensorflow.keras.layers import Dot
from tensorflow.keras.utils import plot_model
from IPython.display import SVG
~~~



### 09-05. 글로브 (GloVe)

**글로브 (Global Vectors for Word Representation)**

- 카운트 기반과 예측 기반을 모두 사용하는 방법론 



#### 1. 기존 방법론에 대한 비판

- LSA : 각 문서에서의 단어의 빈도수를 카운트 한 행렬, 전체적인 통계 정보를 입력으로 받아 차원을 축소하여 잠재된 의미를 끌어내는 방법론
  - 단어 의미의 유추작업에는 성능이 떨잊.ㅁ 
- Word2Vec : 실제값과 예측값에 대한 오차를 손실함수를 통해 줄여나가며 학습하는 예측 기반의 방법론 
  - 윈도우 크기 내에서만 주변 단어를 고려하기에 코퍼스의 전체적인 통계 정보를 반영하지 못함. 



#### 2. 윈도우 기반 동시 등장 행렬 (Window based Co-occurerence Matrix)

**단어의 동시 등장 행렬**

- 행과 열을 전체 단어 집합의 단어들로 구성
- i 단어의 윈도우 크기 (Window Size) 내에서 k 단어가 등장한 횟수를 i 행 k 열에 기재한 행렬
  - i 단어의 k 단어가 등장한 빈도는 k 단어의 윈도우 크기 내에서 i 단어가 등장한 빈도가 같기에 **Transpose** 해도 동일한 행렬이 됨.



#### 3. 동시 등장 확률 (Co-occurrence Probability)

- 동시 등장 확률 `P(k | i)` : 동시 등장 행렬으로부터 특정 단어 i의 전체 등장 횟수를 카운트하고, 특정 단어 i 가 등장할 때 어떤 단어 k가 등장한 횟수를 카운트하여 계산한 조건부 확률 



#### 4. 손실 함수 (Loss function)

- 용어 설명 

<!-- 이미지 6 추가 -->



**GloVe** : **임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 확률이 되도록 만드는 것**

- 함수 F는 두 단어 사이의 동시 등장 확률의 크기 관계 비 정보를 벡터 공간에 인코딩하는 것이 목적
  - w_i, w_j 라는 두 벡터의 차이를 함수 F의 입력으로 사용하는 것을 제안 
  - 우변은 스칼라, 좌변은 벡터로, F의 입력에 내적을 수행하여 스칼라값으로 만듬 
  - 중심단어와 주변 단어의 선택 기준은 무작위 선택이기 때문에, 자유롭게 교환 될 수 있도록 F 를 실수의 덧셈과 양수의 곱셈에 대한 **준동형 (Homomorphism)**을 만족하도록 함. 


$$
w_i^{\top} \tilde{w}_k + b_i + \tilde{b}_k = \log X_{ik}
$$
**Loss Function의 핵심** 

- GloVe 손실 함수에서 사용하는 가중치 함수 
  -  동시 출현 빈도가 낮은 단어 쌍에 낮은 가중치를 부여하고, 동시 출현 빈도가 높은 단어 쌍에 높은 가중치를 부여함. 



### 09-06. 패스트텍스트 (FastText)

*매커니즘 자체는 Word2Vec의 확장*

- 하나의 단어 안에서도 여러 단어들이 존재하는 것으로 간주



#### 1. 내부 단어 (subword)의 학습

- 각 단어는 글자 단위 n-gram 구성으로 취급 
  - n을 몇으로 결정하는지에 따라 단어들이 얼마나 분리되는지 결정 

~~~
예시)
# n = 3인 경우
<ap, app, ppl, ple, le>, <apple>
~~~

*실제 사용시에는 n의 최솟값과 최대값의 범위를 설정하는데, 기본값으로 3과  6으로 설정되어있음.*



#### 2. 모르는 단어 (Out Of Vocabulary, OOV) 에 대한 대응

- 데이터셋만 충분하면 내부 단어를 통해 모르는 단어(OOV)에 대해서도 다른 단어와의 유사도를 계산이 가능함. 



#### 3. 단어 집합 내 빈도 수가 적었던 단어 (Rare Word)에 대한 대응

- Word2Vec 의 경우 : 등장 빈도수가 적은 단어에 대해서 임베딩의 정확도가 높지 않았다는 단점이 존재 
- FastText 경우 : 단어가 희귀 단어라도, 그 단어의 n-gram이 다른 단어의 n-gram과 겹치는 경우라면, 높은 임베딩 벡터값을 가질 수 있음. 
  - 실제 많은 비정형데이터에서는 오타가 섞여있다. 
  - (이는 Rare Word의 이유가 됨.)



#### 5. 한국에서의 FastText

**음절단위, 자모단위** 총 2가지 기준 

- 자모 단위가 더 오타에 최적화되어 있다고 함. 

> 09-07. 은 유료버전이라 패스



### 09-08. 사전 훈련된 워드 임베딩 (Pre-trained Word Embedding)

#### 1. 케라스 임베딩 층 (Keras Embedding Layer)

- 케라스는 훈련 데이터 단어들에 대한 워드 임베딩을 수행하는 도구로 `Embedding()`을 제공



**1. 임베딩 층은 룩업 테이블이다.**

- 입력 시퀀스의 각 단어들은 모두 정수 인코딩이 되어 있어야함. 

~~~
단어 -> 단어에 부여된 고유한 정수값 -> 임베딩 층 통과 -> 밀집 벡터
~~~

임베딩 층은 밀집 벡터로 맵핑, 가중치가 학습되는 것과 같은 방식으로 훈련

<!-- 이미지 7 추가 -->



- 특정 단어와 맵핑되는 정수를 인덱스로 가지는 테이블로부터 임베딩 벡터 값을 가져오는 룩업 테이블로 본다. 
- 단어 집합의 크기만큼의 행을 가지기에 모든 단어는 고유한 임베딩 벡터를 가진다. 



**2. 임베딩 층 사용하기**

- 학습 과정에서 현재 각 단어들의 임베딩 벡터들의 값은 출력층의 가중치와 함께 학습됨. 



### 09-09. 엘모 (Embeddings from Language Model, ELMo)

**ELMo** : 새로운 워드 임베딩 방법론 

- **사전 훈련된 언어 모델 (Pre-trained language model)**을 사용한다. 



#### 1. ELMo (Embeddings from Language Model)

같은 표기의 단어라도 문맥에 따라서 다르게 워드 임베딩을 할 수 있으며 자연어 처리의 성능을 올릴 수 있다. 

=> 따라서 필요한게 워드 임베딩 시 문맥을 고려해서 임베딩을 하겠다는 아이디어 **문맥을 반영한 워드 임베딩** 



#### 2. biLM (Bidirectional Language Model)의 사전훈련

- RNN 언어 모델은 문장으로부터 단어 단위로 입력을 받는데, RNN 내부의 은닉상태는 시점이 지날수록 업데이트가 되간다. 
- 다층 구조를 전제 (은닉층이 최소 2개 이상)
- **양방향 RNN과 biLM은 다르다.**
  - 양방향은 순방향 RNN의 은닉상태와 역방향의 RNN의 은닉상태를 연결하여 다음층의 입력으로 사용
  - biLM의 순방향 언어모델과 역방향 언어모델이라는 두 개의 언어 모델을 별개의 모델로 보고 학습 



#### 3. biLM의 활용

- 해당 시점(time step)의 BiLM의 각 층의 출력값을 가져온다. 
  - 첫번째는 임베딩 층
  - 나머지 층 : 각 층의 은닉상태 

**ELMO가 임베딩 벡터를 얻는 과정**

1. **각 층의 출력값을 연결 (concatenate) 한다.**

2. **각 층의 출력값 별로 가중치를 준다.**
3. **각 층의 출력값을 모두 더한다.**
   - 2,3 번의 단계를 요약해 가중합을 한다고 말함.
4. **벡터의 크기를 결정하는 스칼라 매개변수를 곱한다.**



> 09-10 부터 09-14까지는 실습이라서 그냥 보면서 이해함...
>
> 스팸 메일 필터링이 많이 신기하네요..



### 10장 ~ 11장 중요해보이는 부분들 정리 

















