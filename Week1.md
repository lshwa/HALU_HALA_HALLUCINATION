# 딥러닝을 이용한 자연어 처리 입문

`2장, 7장, 8장`을 중심으로 공부하되, 다른 장도 읽어보면서 중요한 것들 위주 정리

> LLM 을 큰 주제로 본다면 배워야할 것 : 이론적 위주
>
> - 트랜스포머 (Transformer)
> - BERT, GPT, BART, T5, GPT-3
> - PEFT(Parameter-Efficient Fine-Tuning), LLama



### 1장. 자연어 처리 (natural language processing)

**자연어(natural language)** : 일상 생활에서 사용하는 언어

**자연어 처리** : 자연어의 의미를 분석하여 컴퓨터가 처리할 수 있도록 하는 일 



- 필요 프레임워크 , 라이브러리 

> 필요한 패키지 
>
> - 코랩 (Colab) or Anaconda : 이미 설치 완료
> - Tensorflow : 구글에서 공개한 머신 러닝 오픈소스 라이브러리 
> - Keras : 딥러닝 프레임워크인 텐서플로우에 API 제공
> - Gensim : 머신러닝을 사용해 토픽 모델링과 자연어 처리를 수행을 도와주는 오픈 소스 라이브러리
> - Scikit-learn : 파이썬 머신러닝 라이브러리
> - KoNLPy : 한국어 자연어 처리를 위한 형태소 분석기 패키지



**머신 러닝 워크플로우 (Machine Learning Workflow)**

![alt text](images/Week1/image1.png)

1. **수집 (Acquisition)**

- 기계에 학습시킬 데이터를 필요로 함. 자연어 데이터를 **말뭉치, 코퍼스** 라고 부르는데 조사나 연구 목적에 의해 특정 도메인으로부터 수집된 텍스트 집합을 말함. 
- 주로 txt, csv, xml 파일들로 다양하고 음성 데이터, 웹 수집기를 통해 수집함.



2. **점검 및 탐색 (Inspection and exploration)**

- 데이터를 점검하고 탐색하는 단계 
- **EDA (탐색적 데이터 분석)** 단계라고도 함. 독립, 종속, 변수 유형, 변수의 데이터 타입을 점검하며 데이터의 특징과 내재하는 구조적 관계를 알아내는 과정



3. **전처리 및 정제 (Preprocessing and Cleaning)**

- 데이터의 전처리 과정 (제일 까다로운 작업)
- 자연어 처리 : 토큰화, 정제, 정규화, 불용어 제거 등의 단계가 포함 



4. **모델링 및 훈련 (Modeling and Training)**

- 적절한 머신러닝 알고리즘을 선택해, 전처리가 완료된 데이터를 머신러닝 알고리즘을 통해 기계에게 학습을 시킴. 
- 그 이후 기계 번역, 음성 인식, 텍스트 분류 등의 자연어 처리 작업을 수행할 수 있게 함. 

- 훈련용, 테스트용, 검증용 세 가지 모두 사용함. 



5. **평가 (Evaluation)**

- 테스트용 데이터로 성능을 평가를 진행함.



6. **배포 (Deployment)**

- 완성된 모델 배포하는 단계
- 피드백을 받으면서 모델을 업데이트 해야하는 상황으로 온다면 수집단계로 돌아갈 수도 있음.



---

## 02장. 텍스트 전처리 (Text Preprocessing)

- 문제의 용도에 맞게 텍스트를 사전에 처리하는 작업 



### 02-01. 토큰화 (Tokenization)

토큰의 기준을 단어(word)로 하는경우, 단어 토큰화라고 함. 

~~~
예시)
Input : Time is an illusion. Lunchtime double so!
Result : "Time", "is", "an", "illustion", "Lunchtime", "double", "so"
~~~

> 띄어쓰기를 기준으로 잘라내는 간단한 예시 
>
> - 하지만 실제로는 그렇게 단순하지는 않음.



- 아포스트로피(') 가 들어가 있는 문장의 예시 

**Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.**

에서 Don't 와 Jone's 는 다양한 방식으로 토큰화가 가능함. 



- word_tokenize : Don't  -> Do /  n't ,  Jone's -> Jone / 's 로 분리 

- wordPunctTokenizer : Dont't -> Don / ' / t , Jone's -> Jone / ' / s 로 분리

- 그 외에도 사용자가 원하는 결과가 나오도록 **토큰화 도구를 직접 설계도 가능**



**토큰화해서 고려해야 할 사항**

**1. 구두점이나 특수 문자를 단순 제외해서는 안된다.**

- 갖고 있는 코퍼스에서 단어들을 걸러낼 때, 구두점이나 특수 문자를 단순히 제외하는 것은 옳지 않다.
  - 마침표(.)와 같은 경우는 문장의 경계를 알 수 있는데 도움이 되기 때문에 제외하지 않을 수 있음.
- 단어 자체가 구두점을 갖고 있는 경우도 있음. 
  - 특수문자(m.p.h) 나 달러나 슬래시같은 예시
  - / 를 활용하여 날짜를 의미하기도 함. 
  - 숫자에 들어가는 쉼표도 예시에 포함될 수 있음. 



**2. 줄임말과 단어 내에 띄어쓰기가 있는 경우**

- 토큰화 작업에서 종종 영어권 언어의 아포스트로피(') 는 압축된 단어를 다시 펼치는 역할을 수행함.
- 하나의 단어 사이에 띄어쓰기가 있는 경우에도 하나의 토큰으로 봐야하는 경우가 존재



**3. 표준 토큰화 예제**

- **Penn Treebank Tokenization**
  - 규칙 1 : 하이푼으로 구성된 단어는 하나로 유지한다. 
  - 규칙 2 : doesn't 와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리해준다. 

~~~
예시) 
Input : "Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own."

print : ['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', "n't", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']
~~~



**4. 문장 토큰화 (Sentence Tokenization)**

- 코퍼스 내에서 문장 단위로 구분하는 작업으로 문장 분류라고도 부름. 
  - 단순히 마침표, 물음표, 느낌표로 할 수 없음. (mail 주소, IP 주소 같은 예시)
- 사용하는 코퍼스가 어떤 국적의 언어인지, 해당 코퍼스 내에서 특수문자들이 어떻게 사용되고 있는지에 따라 직접 규칙들을 정의가 가능 
- NLTK에서는 영어 문장의 토큰화를 수행하는 **sent_tokenize**를 지원함. 

- 한국어의 경우에는 **KSS(Korean sentence Splitter)**가 있음. 

~~~
kss의 예시)
Input :'딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?'

print : ['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다.', '이제 해보면 알걸요?']
~~~



**5. 한국어에서의 토큰화의 어려움**

- 한국어의 경우, 띄어쓰기 단위가 되는 단위를 '어절' 이라고 하는데, 어절 토큰화는 한국어 NLP에서 지양되고 있음. 

  - 어절 토큰화 != 단어 토큰화

  

  **교착어**의 특성

- 영어와 다르게 한국에는 '조사' 가 존재함. 
- 한국어는 어절이 독립적인 단어로 구성되는 것이 아니라 조사 등의 무언가가 붙어있는 경우가 많아서 이를 분리해줘야함. 
- **형태소** : 뜻을 가진 가장 작은 말의 단위 
  - **자립 형태소** : 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소
  - **의존 형태소** : 다른 형태소와 결합하여 사용되는 형태소 



​	**한국어는 띄어쓰기가 영어보다 잘 지켜지지 않는다.**

- 한국어는 띄어쓰기가 지켜지지 않아도 글을 쉽게 이해할 수 있는 언어라는 특성
- 영어와 다르게 띄어쓰기를 하지 않아도 이해가 가능
  - 예시) 제가이렇게띄어쓰기를전혀하지않고글을썼다고하더라도글을이해할수있습니다.



**6. 품사 태깅 (Part-of-speech tagging)**

- 단어의 표기가 같지만, 품사에 따라서 의미가 달라지는 경우도 존재. 
- 단어의 의미를 파악하기 위해 해당 단어가 어떤 품사로 쓰였는지 보는 것이 주요 지표가 될 수 있음. 
- 한국어에서는 KoNLPy 라는 파이썬 패키지를 통해서 형태소 추출과 / 품사 태깅이 가능함. 



### 02-02. 정제 (Cleaning) and 정규화 (Normalization)

- 토큰화 작업 전과 후에는 텍스트 데이터를 용도에 맞게 정제와 정규화하는 일이 항상 함께 있음. 

**정제 (Cleaning)** : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거한다.

- 보통 완벽하게 정제 작업을 할 수가 없어서 대부분 이 정도면 됐다라는 합의점을 찾음.

**정규화 (Normalization)** : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어준다. 



**1. 대소문자 통합**

- 영어권 언어에서 대, 소문자를 통합하는 것은 단어의 개수를 줄일 수 있는 정규화 방법
- 물론, 무작정 통합은 하면 안됨. -> US, us 는 다른뜻을 가짐. 



**2. 불필요한 단어의 제거**

**noise data** : 자연어가 아니면서 아무 의미도 갖지 않는 글자들 (특수 문자 등) 또는 분석하고자 하는 목적에 맞지 않는 불필요한 단어들 

- **등장 빈도가 적은 단어** 
- **길이가 짧은 단어**
  - 영어권 언어에서 길이가 짧은 단어들은 대부분 불용어에 해당 
- **정규 표현식 (Regular Expression)**



### 02-03. 어간 추출 (Stemming) and 표제어 추출(Lemmatization)

하나의 단어로 일반화를 시켜서 문서 내의 단어 수를 줄이겠다는 뜻을 가짐. 

**1. 표제어 추출 (Lemmatization)**

- 단어들로부터 표제어를 찾아가는 과정
  - am, are, is => `be`
- 방법 : 단어의 형태학적 파싱을 먼저 진행하는 것 
  - **어간 / 접사**로 구분을 함. 



**2. 어간 추출 (Stemming)**

- 말 그대로 어간을 추출하는 작업 
  - 형태학적 분석을 단순화한 버전 
- 표제어 추출보다는 어간 추출 속도가 더 빠름. 

> **한국어에서의 어간 추출**
>
> - 활용(conjugation) : 활용 -> 용언의 어간이 어미를 가지는 일
> - 규칙 활용 : 어간이 어미를 취할 때, 어간의 모습이 일정 
> - 불규칙 활용 : 어간이 어미를 취할 때 어간의 모습이 바뀌거나 취하는 어미가 특수한 어미일 경우를 말함. 



### 02-04. 불용어 (Stopword)

자주 등장은 하지만 분석을 하는 것에 도움이 되지 않는 단어들을 제거하는 작업이 필요함. 

- 이러한 단어들을 **불용어 (Stopword)** 라고 부름. 
- 한국어에서의 불용어 제거 방법
  - 토큰화 후에 조사, 접속사 등을 제거하는 방법
  - 사용자가 직접 불용어 사전을 만들게 되는 경우도 존재.
  - 너무 많은 경우 : txt 파일이나, csv 파일에 정리해놓고 불러와서 사용도 함.



### 02-05. 정규 표현식 (Regular Expression)

파이썬에서는 정규 표현식 모듈 `re` 를 지원, 특정 규칙이 있는 텍스트 데이터를 빠르게 정제가 가능

- 정규 표현식 문법 

![alt text](images/Week1/image2.png)

![alt text](images/Week1/image3.png)

- 정규 표현식 모듈 함수 

![alt text](images/Week1/image4.png)

(1) `re.match()` 와  `re.search()` 의 차이

- search() 는 정규 표현식 전체에 대해서 문자열이 매치하는지를 확인

- Match() 는 문자열의 첫 부분부터 정규 표현식과 매치하는지를 확인 



(2) `re.split()`

- 입력된 정규 표현식을 기준으로 문자열들을 분리하여 리스트로 리턴
  - 토큰화에 유용하게 주로 쓰임. 



(3) `re.findall()`

- 정규표현식과 매치되는 모든 문자열들을 리스트로 리턴
- 매치되는 문자열이 없는 경우네는 빈 리스트를 리턴



(4) `re.sub()`

- 정규 표현식과 일치하는 문자열을 찾아 다른 문자열로 대체



### 02-06. 정수 인코딩(Integer Encoding)

컴퓨터는 텍스트보다 숫자를 더 잘 처리하기에 텍스트를 숫자로 바꾸는 기법들이 존재

**1. 정수 인코딩 (Integer Encoding)**

- 단어에 정수를 부여하는 방법 중 하나로 단어를 빈도수 순으로 정렬한 단어 집합을 만듦.
- 빈도수가 높은 순서대로 차례로 낮은 숫자부터 부여하는 방법
- Dicionary , Counter , NLTK의 FreqDist, enumerate 를 통해 구현 가능



### 02-07. 패딩 (Padding)

각 문장, 문서의 길이가 서로 다를 수 있다. 

- 즉, 병렬 연산을 위해서 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업이 필요함. 

> 이게 그때 DARtv 학술제때 사용했던 기법 같음. 

- 데이터에 특정 값을 채워서 데이터의 크기를 조정하는 것을 **패딩**이라고 함. 
  - 숫자 0을 사용하고 있다면 **제로 패딩(zero padding)**이라고 함. 

~~~
예시 )
array([[ 1,  5,  0,  0,  0,  0,  0],
       [ 1,  8,  5,  0,  0,  0,  0],
       [ 1,  3,  5,  0,  0,  0,  0],
       [ 9,  2,  0,  0,  0,  0,  0],
       [ 2,  4,  3,  2,  0,  0,  0],
       [ 3,  2,  0,  0,  0,  0,  0],
       [ 1,  4,  6,  0,  0,  0,  0],
       [ 1,  4,  6,  0,  0,  0,  0],
       [ 1,  4,  2,  0,  0,  0,  0],
       [ 7,  7,  3,  2, 10,  1, 11],
       [ 1, 12,  3, 13,  0,  0,  0]])
~~~



### 02-08. 원-핫 인코딩 (One-Hot Encoding)

**단어 집합 (Vocaubulary)** : 서로 다른 단어들의 집합

- 원-핫 인코딩을 수행하기 전 해야할 일 : 단어 집합을 만드는 일
- 단어의 중복을 허용하지 않고 모아놓은 것을 단어 집합. -> 여기에 고유한 정수를 부여하는 정수 인코딩을 진행



​	**원-핫 인코딩 (One-Hot Encoding)**

- 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 벡터 표현 방식 
  - 이때 표현된 벡터는 **원-핫 벡터**라고 함. 
- 두 가지의 과정으로 설명 가능
  - 1. 정수 인코딩을 수행  : 각 단어에 고유한 정수 부여
  - 2. 표현하고 싶은 단어의 고유한 정수를 인덱스로 간주하고 해당 위치에 1부여, 다른 위치에는 0을 부여함. 
- Keras에서는 원-핫 인코딩을 수행하는 도구로 `to_categorical()` 을 지원함. 



**한계**

- 단어의 개수가 늘어날 수록, 벡터를 저장하기 위해 필요한 공간이 계속 늘어남. 
- 단어의 유사도를 표현하지 못함. 
  - 검색 시스템에서 문제가 될 부분
- 해결 방법 : **카운트 기반의 LSA, HAL**  / **예측 기반 벡터화 하는 NNLM, RNNLM, Word2Vec, FastText** 가 존재



### 02-09. 데이터의 분리 (Supervised Learning)

머신 러닝 모델을 학습시키고, 평가하기 위해서는 데이터를 적절하게 분리하는 작업이 필요함. 

**1. 지도 학습 (Supervised Learning)**

- 지도학습의 훈련 데이터는 정답이 무엇인가 맞춰 하는 '문제'에 해당되는 데이터와 레이블이라고 부르는 '정답'이 적혀있는 데이터로 구성되어있다. 

~~~
<훈련 데이터>
X_train : 문제지 데이터
y_train : 문제지에 대한 정답 데이터.

<테스트 데이터>
X_test : 시험지 데이터.
y_test : 시험지에 대한 정답 데이터.
~~~

- 분리하는 작업은 `zip 함수` , `데이터 프레임을 이용한 분리` , `Numpy를 이용한 분리` 가 가능 



### 02-10. 한국어 전처리 패키지 (Text Preprocessing Tools for Korean Text)

- 패키지 정리

~~~
1. PyKospacing
- 띄어쓰기 되어있지 않은 문장을 띄어쓰기를 한 문장으로 변환해주는 패키지 

2. Py-Hanspell
- 현재는 잘 동작하지 않음. (2024-3월 기준)
- 네이버 한글 맞춤법 검사기를 바탕으로 만들어진 패키지 
- 띄어쓰기 또한 보정도 가능

3. SOYNLP를 이용한 단어 토큰화 
- 품사 태깅, 단어 토큰화 등을 지원하는 단어 토크나이저 
- 비지도 학습으로 단어 토큰화를 한다는 특징을 가지고 있음. 
~~~



### 3장 ~ 6장

> 중요해보이는 것들만 간단히 정리 

**언어 모델 (Language Model)을 만드는 방법**

- 언어 모델 : **단어 시퀀스에 확률을 할당(assign)** 하는 일을 하는 모델
  - 다음 단어 시퀀스를 찾아내는 모델, **이전 단어들을 주어졌을 때 다음 단어를 예측**
- **통계를 이용하는 방법, 인공신경망을 이용한 방법** 크게 2가지 존재 



**한국어에서의 언어 모델** ㅠㅠ 

- 한국어는 어순이 중요하지 않음. 
- 한국어는 교착어임. 
- 띄어쓰기가 제대로 지켜지지 않음. 



**PPL : 펄플렉서티 (perplexity)**

- **언어 모델을 평가하기 위한 평가 지표**
  $$
  PPL(W) = P(w_1, w_2, w_3, \ldots, w_N)^{-\frac{1}{N}}
  = \sqrt[N]{\frac{1}{P(w_1, w_2, w_3, \ldots, w_N)}}
  $$

  $$
  PPL(W) = \sqrt[N]{\frac{1}{P(w_1, w_2, \ldots, w_N)}}
  = \sqrt[N]{\frac{1}{\prod_{i=1}^N P(w_i \mid w_1, w_2, \ldots, w_{i-1})}}
  $$

  

**단어 표현의 카테고리화**
![alt text](images/Week1/image5.png)



**Bag of Words (BOW)**

- 단어의 등장 순서를 고려하지 않는 빈도수 기반의 단어 표현 방법

~~~
- BoW 만드는 과정
(1) 각 단어에 고유한 정수 인덱스를 부여합니다.  # 단어 집합 생성.
(2) 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터를 만듭니다.
~~~



**코사인 유사도 (Cosine Similarity)**

- 두 벡터 간의 코사인 각도를 이용하여 구할 수 있는 벡터의 유사도를 의미
- 방향이 동일하면 1, 180 도 면 -1 을 가짐. 

- 그 외 여러가지 문서의 유사도 기법이 존재

~~~
- 유클리드 거리 
- 자카드 유사도 (합집합과 교집합의 비율을 통해 구함)

~~~



## 07. 딥 러닝 (Deep Learning) 의 개요

Deep learning : Machine learning의 한 분야로서 **인공 신경망**의 층을 연속적으로 깊게 쌓아올려 데이터를 학습하는 방식 



### 07-01. 퍼셉트론 (Perceptron)

**퍼셉트론**

- **초기의 인공신경망**
- 다수의 입력으로부터 하나의 결과를 내보내는 알고리즘 

![alt text](images/Week1/image6.png)

- `x` 는 입력 값, `w` 는 가중치 , `y`는 출력값 
- 원은 인공뉴런에 해당
  - 각각의 인공뉴런에서 보내진 입력값 x는 각각의 가중치 w와 함께 종착지인 인공 뉴런에 전달 
  - 가중치가 존재하는데, 가중치의 값이 클수록 해당 입력 값이 중욯다ㅏ는 것을 의미 
  - 각 입력값과 가중치의 곱의 전체 합이 **임계치 (threshold)**를 넘으면 종착치에 있는 인공 뉴런은 1을 출력, 그렇지 않을 경우에는 0을 출력 

- 뉴런에서 출력값을 변경시키는 함수 : **활성화 함수 (Activation Function)**
  - 초기에는 계단 함수를 사용했지만, 그 이후에는 다양한 활성화 함수를 사용함. 

> **로지스틱 회귀 모델** : 인공 신경망에서는 하나의 인공 뉴런
>
> - 로지스틱 회귀를 수행하는 인공 뉴런과 퍼셉트론의 차이는 오직 활성화 함수의 차이 



​	**단층 퍼셉트론 (Single - Layer Perceptron)**

- 값을 보내는 단계랑 값을 받아서 출력하는 두 단계로만 이루어져있음. 
  - 각 단계는 = **층 (layer)** / 입력층과 출력층으로 이루어져 있음.
- AND, NAND, OR 게이트를 구현 가능함. 
- 단 **XOR 게이트는** 구현이 불가능하다. 



​	**다층 퍼셉트론 (MultiLayer Perceptron, MLP)**

![alt text](images/Week1/image7.png)

- 층을 더 쌓으면 만들 수 있다. 
- 단층 : 입력층과 출력층만 존재한다. / 다층 : 중간에 층을 더 추가하였다. 
  - 중간 층 = **은닉층 (hidden layer)**
- 은닉층이 2개 이상인 신경망 -> **심층 신경망 (Deep Neural Network, DNN)**
  - 여러 변형된 다양한 신경망들도 은닉층이 2개 이상이되면 심층 신경망이라고 부름.

> 기계가 가중치를 스스로 찾아내도록 자동화 시키는 단계
>
> **훈련** 또는 **학습**단계라고 부른다. 그리고 **손실함수 (Loss function)**과 **Optimizer**를 사용한다. 

**학습을 시키는 인공 신경망이 심층 신경망일경우 이를 심층 신경망을 학습시킨다고 하여, 딥 러닝**이라고 부른다. 
